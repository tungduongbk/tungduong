{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    import gzip\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train, Y_train = load_mnist('data', kind='train')\n",
    "X_test, Y_test = load_mnist('data', kind='t10k')\n",
    "m_train = X_train.shape[0]\n",
    "m_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "y_train shape: (60000,)\n",
      "X_test shape: (10000, 784)\n",
      "y_test shape: (10000,)\n",
      "Number of training examples: m_train = 59000\n",
      "Number of testing examples: m_test = 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"y_test shape: \" + str(Y_test.shape))\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data\n",
    "m_train = 58000\n",
    "m_validation = 1000\n",
    "\n",
    "mask = list(range(m_train, m_train + m_validation))\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "mask = list(range(m_train))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "mask = list(range(m_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(m_train, -1)\n",
    "X_val = X_val.reshape(m_validation, -1)\n",
    "X_test = X_test.reshape(m_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):    \n",
    "    # Use ReLU as an activation function !!!!!\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):    \n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4): \n",
    "\n",
    "        self.params = {}    \n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)   \n",
    "        self.params['b1'] = np.zeros((1, hidden_size))    \n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)   \n",
    "        self.params['b2'] = np.zeros((1, output_size))\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # Compute the forward pass\n",
    "        scores = None\n",
    "        h1 = ReLU(np.dot(X, W1) + b1)      \n",
    "        out = np.dot(h1, W2) + b2          \n",
    "        scores = out\n",
    "        \n",
    "        if y is None:   \n",
    "            return scores\n",
    "        \n",
    "        # Compute the loss\n",
    "        scores_max = np.max(scores, axis=1, keepdims=True)    # (N,1)\n",
    "        exp_scores = np.exp(scores - scores_max)              # (N,C)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)    # (N,C)\n",
    "        correct_logprobs = -np.log(probs[range(N), y])        # (N,1)\n",
    "        data_loss = np.sum(correct_logprobs) / N\n",
    "        reg_loss = 0.5 * reg * np.sum(W1*W1) + 0.5 * reg * np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        grads = {}\n",
    "        dscores = probs                                 # (N,C)\n",
    "        dscores[range(N), y] -= 1\n",
    "        dscores /= N\n",
    "        dW2 = np.dot(h1.T, dscores)                     # (H,C)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)    # (1,C)\n",
    "        dh1 = np.dot(dscores, W2.T)                     # (N,H)\n",
    "        dh1[h1 <= 0] = 0\n",
    "        dW1 = np.dot(X.T, dh1)                          # (D,H)\n",
    "        db1 = np.sum(dh1, axis=0, keepdims=True)        # (1,H)\n",
    "        dW2 += reg * W2\n",
    "        dW1 += reg * W1\n",
    "        \n",
    "        grads['W1'] = dW1\n",
    "        grads['b1'] = db1\n",
    "        grads['W2'] = dW2\n",
    "        grads['b2'] = db2\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val, learning_rate=1e-3, \n",
    "               learning_rate_decay=0.95, reg=1e-5, mu=0.9, num_epochs=10, \n",
    "               mu_increase=1.0, batch_size=200, verbose=False):   \n",
    "        \n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(int(num_train / batch_size), 1)\n",
    "        \n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        v_W2, v_b2 = 0.0, 0.0\n",
    "        v_W1, v_b1 = 0.0, 0.0\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in range(1, num_epochs * iterations_per_epoch + 1):   \n",
    "            X_batch = None   \n",
    "            y_batch = None \n",
    "            \n",
    "            # Create a random minibatch of training data and labels\n",
    "            sample_index = np.random.choice(num_train, batch_size, replace=True)   \n",
    "            X_batch = X[sample_index, :]          \n",
    "            y_batch = y[sample_index]             \n",
    "            \n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg) \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # Use the gradients to update the parameters of the network\n",
    "            v_W2 = mu * v_W2 - learning_rate * grads['W2']    \n",
    "            self.params['W2'] += v_W2   \n",
    "            v_b2 = mu * v_b2 - learning_rate * grads['b2']    \n",
    "            self.params['b2'] += v_b2   \n",
    "            v_W1 = mu * v_W1 - learning_rate * grads['W1']    \n",
    "            self.params['W1'] += v_W1   \n",
    "            v_b1 = mu * v_b1 - learning_rate * grads['b1']  \n",
    "            self.params['b1'] += v_b1\n",
    "            \n",
    "            if verbose and it % iterations_per_epoch == 0:    \n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "                epoch = it / iterations_per_epoch    \n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()    \n",
    "                val_acc = (self.predict(X_val) == y_val).mean()    \n",
    "                train_acc_history.append(train_acc)    \n",
    "                val_acc_history.append(val_acc)    \n",
    "                print(\"epoch %d / %d: loss %f, train_acc: %f, val_acc: %f\" % \n",
    "                                    (epoch, num_epochs, loss, train_acc, val_acc))\n",
    "                \n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay    \n",
    "                mu *= mu_increase\n",
    "\n",
    "        return {   \n",
    "            'loss_history': loss_history,   \n",
    "            'train_acc_history': train_acc_history,   \n",
    "            'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):    \n",
    "        y_pred = None    \n",
    "        h1 = ReLU(np.dot(X, self.params['W1']) + self.params['b1'])    \n",
    "        scores = np.dot(h1, self.params['W2']) + self.params['b2']    \n",
    "        y_pred = np.argmax(scores, axis=1)    \n",
    "\n",
    "        return {\"pred\":y_pred, \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 50: loss 0.894735, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 2 / 50: loss 0.634145, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 3 / 50: loss 0.581987, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 4 / 50: loss 0.610015, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 5 / 50: loss 0.608159, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 6 / 50: loss 0.590577, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 7 / 50: loss 0.581339, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 8 / 50: loss 0.603324, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 9 / 50: loss 0.597795, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 10 / 50: loss 0.535498, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 11 / 50: loss 0.581492, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 12 / 50: loss 0.606240, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 13 / 50: loss 0.575563, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 14 / 50: loss 0.558123, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 15 / 50: loss 0.542060, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 16 / 50: loss 0.512157, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 17 / 50: loss 0.592378, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 18 / 50: loss 0.560058, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 19 / 50: loss 0.582467, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 20 / 50: loss 0.577139, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 21 / 50: loss 0.549241, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 22 / 50: loss 0.553244, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 23 / 50: loss 0.590131, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 24 / 50: loss 0.560759, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 25 / 50: loss 0.560304, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 26 / 50: loss 0.534490, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 27 / 50: loss 0.572744, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 28 / 50: loss 0.581255, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 29 / 50: loss 0.534775, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 30 / 50: loss 0.532323, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 31 / 50: loss 0.554184, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 32 / 50: loss 0.565924, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 33 / 50: loss 0.557121, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 34 / 50: loss 0.539777, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 35 / 50: loss 0.526413, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 36 / 50: loss 0.529198, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 37 / 50: loss 0.504003, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 38 / 50: loss 0.540169, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 39 / 50: loss 0.531680, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 40 / 50: loss 0.553468, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 41 / 50: loss 0.523221, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 42 / 50: loss 0.568713, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 43 / 50: loss 0.507112, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 44 / 50: loss 0.519259, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 45 / 50: loss 0.549336, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 46 / 50: loss 0.499059, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 47 / 50: loss 0.517702, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 48 / 50: loss 0.505182, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 49 / 50: loss 0.523223, train_acc: 0.000000, val_acc: 0.000000\n",
      "epoch 50 / 50: loss 0.537983, train_acc: 0.000000, val_acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 10\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_epochs=50, batch_size=1024,\n",
    "            learning_rate=7.5e-4, learning_rate_decay=0.95,\n",
    "            reg=1.0, verbose=True)\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val)[\"pred\"] == y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.846\n",
      "[9 2 1 ... 8 1 5]\n"
     ]
    }
   ],
   "source": [
    "test_acc = (net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.7803521779 0.8420000000 0.8100048100      1000\n",
      "           1  0.9793174767 0.9470000000 0.9628876462      1000\n",
      "           2  0.7487386478 0.7420000000 0.7453540934      1000\n",
      "           3  0.8468379447 0.8570000000 0.8518886680      1000\n",
      "           4  0.7284522706 0.7860000000 0.7561327561      1000\n",
      "           5  0.9615795091 0.9010000000 0.9303045947      1000\n",
      "           6  0.6702253855 0.5650000000 0.6131307651      1000\n",
      "           7  0.8978805395 0.9320000000 0.9146221786      1000\n",
      "           8  0.9250243427 0.9500000000 0.9373458313      1000\n",
      "           9  0.9221032132 0.9470000000 0.9343857918      1000\n",
      "\n",
      "   micro avg  0.8469000000 0.8469000000 0.8469000000     10000\n",
      "   macro avg  0.8460511508 0.8469000000 0.8456057135     10000\n",
      "weighted avg  0.8460511508 0.8469000000 0.8456057135     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "sk_report = classification_report(\n",
    "    digits=10,\n",
    "    y_true=y_test, \n",
    "    y_pred=net.predict(X_test)[\"pred\"])\n",
    "print(sk_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.63877639 -3.91037868 -2.75536303 ...  4.65691401  1.86315696\n",
      "   6.07521548]\n",
      " [ 1.78464493 -1.68115125  8.47378017 ... -9.82682757 -0.06180318\n",
      "  -5.813478  ]\n",
      " [ 3.51447419 10.55918111 -0.37047964 ... -3.77161365 -3.55574883\n",
      "  -3.57200202]\n",
      " ...\n",
      " [ 2.41787177 -4.96329511  0.11286479 ... -3.08594095  5.47516198\n",
      "  -3.35863543]\n",
      " [ 0.49367618  8.41073861 -0.1452789  ... -1.89258877 -3.54110489\n",
      "  -1.42568814]\n",
      " [-2.00560723 -1.91923616 -1.41930851 ...  2.91104939  1.75332331\n",
      "   1.08347237]]\n"
     ]
    }
   ],
   "source": [
    "# y_test = net.predict(X_test)[\"scores\"]\n",
    "y_score = net.predict(X_test)[\"scores\"]\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-1798be1b1799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    print(roc_auc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
